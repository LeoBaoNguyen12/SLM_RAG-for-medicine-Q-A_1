{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank-bm25 transformers nltk llama-cpp-python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n",
    "import torch\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_pqau = \"/content/ori_pqau.json\"\n",
    "with open(file_path_pqau, \"r\") as f:\n",
    "    pqa_u = json.load(f)\n",
    "\n",
    "corpus = []\n",
    "for key, value in pqa_u.items():\n",
    "    context = \" \".join(value[\"CONTEXTS\"])\n",
    "    corpus.append(context)\n",
    "\n",
    "tokenized_corpus = [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_classify_path = \"/content/results_biobert_finetuned\"\n",
    "\n",
    "tokenizer_classify = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "model_classify = AutoModelForSequenceClassification.from_pretrained(model_classify_path)\n",
    "model_classify.to(device)\n",
    "\n",
    "print(\"Loading LLaMA 7B model...\")\n",
    "llm = Llama(\n",
    "    model_path=\"/content/llama-2-7b-chat.Q4_0.gguf\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=0,\n",
    "    verbose=False\n",
    ")\n",
    "print(\" Tokenizer v√† c√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_evidence(question, top_k=3):\n",
    "    \"\"\"Retrieve top-k relevant evidence documents with BM25 scores\"\"\"\n",
    "    tokenized_query = nltk.word_tokenize(question.lower())\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    retrieved_docs = [corpus[i] for i in top_indices]\n",
    "    retrieved_scores = [scores[i] for i in top_indices]\n",
    "    return retrieved_docs, retrieved_scores\n",
    "\n",
    "def classify_answer(question, evidence):\n",
    "    \"\"\"Classify answer as yes/no/maybe using fine-tuned model\"\"\"\n",
    "    combined_context = \" \".join(evidence)\n",
    "    inputs = tokenizer_classify(\n",
    "        question,\n",
    "        combined_context,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_classify(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    label = \"yes\" if pred == 0 else \"no\" if pred == 1 else \"maybe\"\n",
    "    return label\n",
    "\n",
    "def generate_comprehensive_answer(question, evidence_docs, evidence_scores):\n",
    "    \"\"\"Generate detailed answer using LLaMA with retrieved evidence\"\"\"\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    for i, (doc, score) in enumerate(zip(evidence_docs, evidence_scores)):\n",
    "        evidence_text += f\"[Document {i+1}, Relevance: {score:.3f}]: {doc}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "You are a professional medical assistant. Answer the question based STRICTLY on the provided evidence.\n",
    "Use only information from the evidence documents below.\n",
    "\n",
    "ANSWER STRUCTURE:\n",
    "1. First, provide a short answer: \"yes\", \"no\", or \"maybe\"\n",
    "2. Then, explain in detail based on the evidence\n",
    "3. Finally, assess the reliability of the evidence\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "EVIDENCE:\n",
    "{evidence_text}\n",
    "<</SYS>>\n",
    "\n",
    "Please answer the question based on the provided evidence. [/INST]\"\"\"\n",
    "\n",
    "    response = llm(\n",
    "        prompt,\n",
    "        max_tokens=512,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        echo=False,\n",
    "        stop=[\"</s>\", \"[INST]\"]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "def answer_biomedical_question(question, top_k_retrieve=3):\n",
    "    \"\"\"Main pipeline for biomedical question answering\"\"\"\n",
    "    print(f\"üîç Question: {question}\")\n",
    "\n",
    "    evidence_docs, evidence_scores = retrieve_evidence(question, top_k=top_k_retrieve)\n",
    "    print(f\"üìö Retrieved {len(evidence_docs)} relevant documents\")\n",
    "\n",
    "    short_answer = classify_answer(question, evidence_docs)\n",
    "    print(f\"‚úÖ Short answer: {short_answer}\")\n",
    "\n",
    "    detailed_answer = generate_comprehensive_answer(question, evidence_docs, evidence_scores)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"short_answer\": short_answer,\n",
    "        \"detailed_answer\": detailed_answer,\n",
    "        \"retrieved_evidence\": evidence_docs,\n",
    "        \"relevance_scores\": evidence_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ee27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_question = \"Does aspirin reduce the risk of heart attack in patients with diabetes?\"\n",
    "\n",
    "    print(\"üöÄ Starting biomedical QA pipeline...\")\n",
    "    result = answer_biomedical_question(test_question)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ FINAL RESULTS:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Short Answer: {result['short_answer']}\")\n",
    "    print(f\"Detailed Answer:\\n{result['detailed_answer']}\")\n",
    "    print(f\"\\nNumber of Retrieved Documents: {len(result['retrieved_evidence'])}\")\n",
    "    for i, (doc, score) in enumerate(zip(result['retrieved_evidence'], result['relevance_scores'])):\n",
    "        print(f\"Document {i+1} (Score: {score:.3f}): {doc[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
