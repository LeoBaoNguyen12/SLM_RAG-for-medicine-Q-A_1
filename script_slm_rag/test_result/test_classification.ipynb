{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c21be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import json\n",
    "import torch\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Download NLTK n·∫øu c·∫ßn\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Load d·ªØ li·ªáu t·ª´ pqa_l (thay v√¨ ori_pqau.json)\n",
    "file_path_pqa_l = \"/content/ori_pqal.json\"  # Gi·∫£ ƒë·ªãnh t√™n file; thay n·∫øu kh√°c\n",
    "try:\n",
    "    with open(file_path_pqa_l, \"r\", encoding=\"utf-8\") as f:\n",
    "        pqa_l = json.load(f)  # D√πng pqa_l thay pqa_u\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File {file_path_pqa_l} not found. Please check the path.\")\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(\"Invalid JSON file.\")\n",
    "\n",
    "# T·∫°o corpus t·ª´ pqa_l\n",
    "corpus = []\n",
    "for key, value in pqa_l.items():\n",
    "    if \"CONTEXTS\" in value and isinstance(value[\"CONTEXTS\"], list):\n",
    "        context = \" \".join(value[\"CONTEXTS\"])\n",
    "        corpus.append(context)\n",
    "print(f\"Corpus size: {len(corpus)}\")\n",
    "\n",
    "# Tokenize cho BM25\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "tokenized_corpus = [simple_tokenize(doc) for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Load models (gi·∫£ ƒë·ªãnh ƒë√£ c√≥ t·ª´ code g·ªëc)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_classify_path = \"/content/results_biobert_finetuned\"\n",
    "tokenizer_classify = AutoTokenizer.from_pretrained(model_classify_path)  # T·ª´ th∆∞ m·ª•c fine-tuned\n",
    "model_classify = AutoModelForSequenceClassification.from_pretrained(model_classify_path)\n",
    "model_classify.to(device)\n",
    "\n",
    "# H√†m retrieve_evidence\n",
    "def retrieve_evidence(question, top_k=3):\n",
    "    if not corpus:\n",
    "        return []\n",
    "    tokenized_query = simple_tokenize(question)\n",
    "    top_indices = bm25.get_top_n(tokenized_query, range(len(corpus)), n=top_k)\n",
    "    return [corpus[i] for i in top_indices]\n",
    "\n",
    "# H√†m classify_answer\n",
    "def classify_answer(question, evidence):\n",
    "    if not evidence:\n",
    "        return \"maybe\"\n",
    "    combined_input = question + \" [SEP] \" + \" \".join(evidence[:2])\n",
    "    inputs = tokenizer_classify(\n",
    "        combined_input,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_classify(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return [\"yes\", \"no\", \"maybe\"][pred]\n",
    "\n",
    "# Load test_ground_truth\n",
    "test_file_path = \"/content/test_ground_truth.json\"\n",
    "try:\n",
    "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        test_ground_truth = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File {test_file_path} not found.\")\n",
    "\n",
    "# Ki·ªÉm tra common keys gi·ªØa pqa_l v√† test_ground_truth\n",
    "common_keys = set(pqa_l.keys()) & set(test_ground_truth.keys())\n",
    "print(f\"Number of common keys between pqa_l and test_ground_truth: {len(common_keys)}\")\n",
    "if len(common_keys) == 0:\n",
    "    print(\"Still no common keys! Please check file names or structures.\")\n",
    "else:\n",
    "    print(\"Common keys found! Proceeding with evaluation.\")\n",
    "\n",
    "# Danh s√°ch predictions v√† ground_truths\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "labels = [\"yes\", \"no\", \"maybe\"]\n",
    "\n",
    "print(\"üîç Starting evaluation of classification on test set...\")\n",
    "for test_id, true_label in test_ground_truth.items():\n",
    "    if test_id not in pqa_l:\n",
    "        print(f\"Warning: ID {test_id} not found in pqa_l. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    question = pqa_l[test_id].get(\"QUESTION\", \"\")\n",
    "    if not question:\n",
    "        print(f\"Warning: No question found for ID {test_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    evidence_docs = retrieve_evidence(question, top_k=3)\n",
    "    pred_label = classify_answer(question, evidence_docs)\n",
    "\n",
    "    predictions.append(pred_label)\n",
    "    ground_truths.append(true_label)\n",
    "\n",
    "    print(f\"ID: {test_id} | Predicted: {pred_label} | Ground Truth: {true_label}\")\n",
    "\n",
    "# T√≠nh metrics\n",
    "accuracy = accuracy_score(ground_truths, predictions)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(ground_truths, predictions, labels=labels, average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(ground_truths, predictions, labels=labels, average='macro')\n",
    "micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(ground_truths, predictions, labels=labels, average='micro')\n",
    "\n",
    "print(\"\\nüìä CLASSIFICATION EVALUATION RESULTS:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro Recall: {micro_recall:.4f}\")\n",
    "print(f\"Micro F1-Score: {micro_f1:.4f}\")\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}, Support={support[i]}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(ground_truths, predictions, labels=labels, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"/content/results/\"  \n",
    "os.makedirs(results_dir, exist_ok=True)  \n",
    "metrics_file = os.path.join(results_dir, \"classification_metrics.txt\")\n",
    "with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"üìä CLASSIFICATION EVALUATION RESULTS:\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"Macro Precision: {macro_precision:.4f}\\n\")\n",
    "    f.write(f\"Macro Recall: {macro_recall:.4f}\\n\")\n",
    "    f.write(f\"Macro F1-Score: {macro_f1:.4f}\\n\")\n",
    "    f.write(f\"Micro Precision: {micro_precision:.4f}\\n\")\n",
    "    f.write(f\"Micro Recall: {micro_recall:.4f}\\n\")\n",
    "    f.write(f\"Micro F1-Score: {micro_f1:.4f}\\n\\n\")\n",
    "    f.write(\"Per-Class Metrics:\\n\")\n",
    "    for i, label in enumerate(labels):\n",
    "        f.write(f\"{label}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}, Support={support[i]}\\n\")\n",
    "    f.write(\"\\nDetailed Classification Report:\\n\")\n",
    "    f.write(classification_report(ground_truths, predictions, labels=labels, target_names=labels))\n",
    "print(f\" Classification metrics saved to: {metrics_file}\")\n",
    "predictions_file = os.path.join(results_dir, \"predictions.json\")\n",
    "pred_data = {\n",
    "    \"predictions\": predictions,\n",
    "    \"ground_truths\": ground_truths,\n",
    "    \"test_ids\": list(test_ground_truth.keys())  \n",
    "}\n",
    "with open(predictions_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pred_data, f, indent=4)\n",
    "print(f\" Predictions saved to: {predictions_file}\")\n",
    "print(f\" All results saved in directory: {results_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e408dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
