{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_rag_answer(question):\n",
    "    \"\"\"Generate answer using only LLaMA's internal knowledge without retrieval\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "You are a professional medical assistant with expertise in biomedical knowledge.\n",
    "Answer the following biomedical question to the best of your ability based on your training.\n",
    "\n",
    "ANSWER STRUCTURE:\n",
    "1. First, provide a short answer: \"yes\", \"no\", or \"maybe\"\n",
    "2. Then, explain in detail based on your biomedical knowledge\n",
    "3. If you're uncertain, please indicate the limitations of your knowledge\n",
    "\n",
    "QUESTION: {question}\n",
    "<</SYS>>\n",
    "\n",
    "Please answer the biomedical question based on your knowledge. [/INST]\"\"\"\n",
    "\n",
    "    response = llm(\n",
    "        prompt,\n",
    "        max_tokens=512,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        echo=False,\n",
    "        stop=[\"</s>\", \"[INST]\"]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "def classify_non_rag_answer(question):\n",
    "    \"\"\"Classify answer using only the question (no evidence)\"\"\"\n",
    "    inputs = tokenizer_classify(\n",
    "        question,\n",
    "        \"\",  \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_classify(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    label = \"yes\" if pred == 0 else \"no\" if pred == 1 else \"maybe\"\n",
    "    return label\n",
    "\n",
    "def answer_biomedical_question_non_rag(question):\n",
    "    \"\"\"Non-RAG pipeline for biomedical question answering\"\"\"\n",
    "    print(f\"üîç Question: {question}\")\n",
    "\n",
    "    short_answer = classify_non_rag_answer(question)\n",
    "    print(f\" Short answer: {short_answer}\")\n",
    "\n",
    "    detailed_answer = generate_non_rag_answer(question)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"short_answer\": short_answer,\n",
    "        \"detailed_answer\": detailed_answer,\n",
    "        \"retrieved_evidence\": [],\n",
    "        \"relevance_scores\": [],\n",
    "        \"method\": \"non_rag\"\n",
    "    }\n",
    "\n",
    "def compare_rag_vs_non_rag(question):\n",
    "    \"\"\"So s√°nh k·∫øt qu·∫£ gi·ªØa RAG v√† Non-RAG\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ü§ñ COMPARISON: RAG vs NON-RAG\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nüîÑ RAG APPROACH:\")\n",
    "    rag_result = answer_biomedical_question(question)\n",
    "\n",
    "    print(\"\\n‚ö° NON-RAG APPROACH:\")\n",
    "    non_rag_result = answer_biomedical_question_non_rag(question)\n",
    "\n",
    "    return {\n",
    "        \"rag\": rag_result,\n",
    "        \"non_rag\": non_rag_result\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_question = \"Does smoking cause lung cancer?\"\n",
    "\n",
    "    print(\" RUNNING NON-RAG VERSION ONLY:\")\n",
    "    non_rag_result = answer_biomedical_question_non_rag(test_question)\n",
    "\n",
    "    print(\"\\nüìä FINAL NON-RAG RESULT:\")\n",
    "    print(f\"Question: {non_rag_result['question']}\")\n",
    "    print(f\"Short answer: {non_rag_result['short_answer']}\")\n",
    "    print(f\"Detailed answer: {non_rag_result['detailed_answer']}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
