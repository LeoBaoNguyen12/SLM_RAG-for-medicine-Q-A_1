{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üì± Using device: {device}\")\n",
    "\n",
    "model_classify_path = \"/content/results_biobert_finetuned\"\n",
    "\n",
    "print(\" Loading tokenizer and model...\")\n",
    "tokenizer_classify = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "model_classify = AutoModelForSequenceClassification.from_pretrained(model_classify_path)\n",
    "model_classify.to(device)\n",
    "model_classify.eval()\n",
    "print(\" Model loaded successfully!\")\n",
    "\n",
    "def classify_non_rag_answer(question):\n",
    "    \"\"\"Ph√¢n lo·∫°i c√¢u tr·∫£ l·ªùi ch·ªâ d·ª±a tr√™n c√¢u h·ªèi, kh√¥ng d√πng RAG\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer_classify(\n",
    "            question,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_classify(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "        return [\"yes\", \"no\", \"maybe\"][pred]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Classification error: {e}\")\n",
    "        return \"maybe\"  \n",
    "\n",
    "def load_pqa_questions(pqa_file_path):\n",
    "    \"\"\"Load c√¢u h·ªèi t·ª´ file PQA g·ªëc\"\"\"\n",
    "    try:\n",
    "        with open(pqa_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pqa_data = json.load(f)\n",
    "        print(f\" Loaded PQA data: {len(pqa_data)} items\")\n",
    "        return pqa_data\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading PQA data: {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_test_ground_truth(ground_truth_file_path):\n",
    "    \"\"\"Load ground truth t·ª´ file test\"\"\"\n",
    "    try:\n",
    "        with open(ground_truth_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ground_truth = json.load(f)\n",
    "        print(f\" Loaded ground truth: {len(ground_truth)} items\")\n",
    "        return ground_truth\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading ground truth: {e}\")\n",
    "        return {}\n",
    "\n",
    "def map_questions_to_ground_truth(pqa_data, ground_truth):\n",
    "    \"\"\"Map ID t·ª´ ground truth v·ªõi c√¢u h·ªèi t·ª´ PQA data\"\"\"\n",
    "    mapped_data = []\n",
    "    not_found_count = 0\n",
    "\n",
    "    for id, true_label in ground_truth.items():\n",
    "        if id in pqa_data:\n",
    "            question = pqa_data[id].get(\"QUESTION\", \"\")\n",
    "            if question:  \n",
    "                mapped_data.append({\n",
    "                    \"id\": id,\n",
    "                    \"question\": question,\n",
    "                    \"true_label\": true_label\n",
    "                })\n",
    "        else:\n",
    "            not_found_count += 1\n",
    "\n",
    "    if not_found_count > 0:\n",
    "        print(f\" {not_found_count} IDs not found in PQA data\")\n",
    "\n",
    "    return mapped_data\n",
    "\n",
    "def evaluate_classification_model(test_data, sample_size=None):\n",
    "    \"\"\"ƒê√°nh gi√° classification model v·ªõi c√¢u h·ªèi th·∫≠t (kh√¥ng RAG)\"\"\"\n",
    "\n",
    "    if sample_size and sample_size < len(test_data):\n",
    "        import random\n",
    "        random.seed(42)  \n",
    "        test_data = random.sample(test_data, sample_size)\n",
    "\n",
    "    print(f\" Evaluating on {len(test_data)} samples (without RAG)...\")\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for i, item in enumerate(test_data):\n",
    "        try:\n",
    "            question = item[\"question\"]\n",
    "            true_label = item[\"true_label\"]\n",
    "\n",
    "            pred_label = classify_non_rag_answer(question)\n",
    "\n",
    "            true_labels.append(true_label)\n",
    "            pred_labels.append(pred_label)\n",
    "\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\" Processed {i+1}/{len(test_data)} samples\")\n",
    "                print(f\"   Sample: Q: '{question[:60]}...'\")\n",
    "                print(f\"   True: {true_label} | Pred: {pred_label}\")\n",
    "\n",
    "            if (i + 1) % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error on sample {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"üìã Successfully processed {len(true_labels)} samples\")\n",
    "    return true_labels, pred_labels\n",
    "\n",
    "def calculate_metrics(true_labels, pred_labels):\n",
    "    \"\"\"T√≠nh c√°c metrics ƒë√°nh gi√°\"\"\"\n",
    "\n",
    "    if len(true_labels) == 0:\n",
    "        print(\"‚ùå No data to evaluate\")\n",
    "        return 0, None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä CLASSIFICATION EVALUATION RESULTS (NON-RAG)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    print(f\"üéØ Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "    print(\"\\nüìà Detailed Classification Report:\")\n",
    "    print(classification_report(true_labels, pred_labels, target_names=[\"yes\", \"no\", \"maybe\"]))\n",
    "\n",
    "    print(\"\\nüîÑ Confusion Matrix:\")\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=[\"yes\", \"no\", \"maybe\"])\n",
    "    print(\"True \\\\ Pred |   yes  |   no   | maybe \")\n",
    "    print(\"-\" * 45)\n",
    "    for i, true_label in enumerate([\"yes\", \"no\", \"maybe\"]):\n",
    "        row = \"   {:5s}   |\".format(true_label)\n",
    "        for j in range(3):\n",
    "            row += \"  {:4d}  |\".format(cm[i][j])\n",
    "        print(row)\n",
    "\n",
    "    errors = sum(1 for true, pred in zip(true_labels, pred_labels) if true != pred)\n",
    "    print(f\"\\n Total Errors: {errors}/{len(true_labels)}\")\n",
    "    print(f\" Correct Predictions: {len(true_labels)-errors}/{len(true_labels)}\")\n",
    "\n",
    "    true_dist = Counter(true_labels)\n",
    "    pred_dist = Counter(pred_labels)\n",
    "    print(f\"\\nüìä Class Distribution - True: {dict(true_dist)}\")\n",
    "    print(f\"üìä Class Distribution - Pred: {dict(pred_dist)}\")\n",
    "\n",
    "    return accuracy, cm\n",
    "\n",
    "def main_classification_evaluation():\n",
    "    \"\"\"H√†m ch√≠nh ƒë·ªÉ ch·∫°y ƒë√°nh gi√° classification kh√¥ng d√πng RAG\"\"\"\n",
    "\n",
    "    print(\"üìÅ Loading data...\")\n",
    "\n",
    "    pqa_data = load_pqa_questions(\"/content/ori_pqal.json\")\n",
    "\n",
    "    ground_truth = load_test_ground_truth(\"/content/test_ground_truth.json\")\n",
    "\n",
    "    if not pqa_data or not ground_truth:\n",
    "        print(\" Cannot load data files\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìä PQA data: {len(pqa_data)} items\")\n",
    "    print(f\"üìä Ground truth: {len(ground_truth)} items\")\n",
    "\n",
    "    print(\"üîó Mapping questions to ground truth...\")\n",
    "    test_data = map_questions_to_ground_truth(pqa_data, ground_truth)\n",
    "\n",
    "    print(f\"üìã Mapped {len(test_data)} samples\")\n",
    "\n",
    "    if len(test_data) == 0:\n",
    "        print(\" No samples mapped! Check file paths and data structure.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüîç Sample mapped data:\")\n",
    "    for i in range(min(3, len(test_data))):\n",
    "        sample = test_data[i]\n",
    "        print(f\"  {i+1}. ID: {sample['id']}\")\n",
    "        print(f\"     Q: {sample['question'][:80]}...\")\n",
    "        print(f\"     A: {sample['true_label']}\")\n",
    "\n",
    "    sample_size = min(200, len(test_data))\n",
    "    print(f\"\\nüî¨ Using sample size: {sample_size}\")\n",
    "\n",
    "    true_labels, pred_labels = evaluate_classification_model(test_data, sample_size)\n",
    "\n",
    "    if len(true_labels) == 0:\n",
    "        print(\" No valid samples to evaluate!\")\n",
    "        return None\n",
    "\n",
    "    accuracy, cm = calculate_metrics(true_labels, pred_labels)\n",
    "\n",
    "    results = {\n",
    "        \"sample_size\": len(true_labels),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"class_names\": [\"yes\", \"no\", \"maybe\"],\n",
    "        \"data_source\": \"PQA mapped questions\",\n",
    "        \"model_type\": \"Non-RAG Classification\",\n",
    "        \"model_config\": {\n",
    "            \"base_model\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
    "            \"fine_tuned_path\": model_classify_path,\n",
    "            \"device\": str(device)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"non_rag_classification_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"\\nüíæ Results saved to non_rag_classification_results.json\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ STARTING NON-RAG CLASSIFICATION EVALUATION\")\n",
    "    results = main_classification_evaluation()\n",
    "\n",
    "    if results:\n",
    "        print(\" Evaluation completed successfully!\")\n",
    "    else:\n",
    "        print(\" Evaluation failed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
