{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama\n",
    "\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "print(\"üîÑ Initializing models...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üì± Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    model_classify_path = \"/content/results_biobert_finetuned\"\n",
    "    tokenizer_classify = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "    model_classify = AutoModelForSequenceClassification.from_pretrained(model_classify_path)\n",
    "    model_classify.to(device)\n",
    "    model_classify.eval()\n",
    "    print(\" Classification model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\" Failed to load classification model: {e}\")\n",
    "    tokenizer_classify = None\n",
    "    model_classify = None\n",
    "\n",
    "try:\n",
    "    print(\"ü¶ô Loading LLaMA model...\")\n",
    "    llm = Llama(\n",
    "        model_path=\"/content/llama-2-7b-chat.Q4_0.gguf\",\n",
    "        n_ctx=2048,\n",
    "        n_threads=8,\n",
    "        n_gpu_layers=0,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\" LLaMA model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load LLaMA model: {e}\")\n",
    "    llm = None\n",
    "\n",
    "def robust_non_rag_evaluation():\n",
    "    \"\"\"ƒê√°nh gi√° Non-RAG th·ª±c t·∫ø nh∆∞ng c√≥ fallback\"\"\"\n",
    "\n",
    "    if tokenizer_classify is None or model_classify is None or llm is None:\n",
    "        print(\"‚ùå Models not properly initialized. Using manual evaluation.\")\n",
    "        return manual_non_rag_evaluation()\n",
    "\n",
    "    try:\n",
    "        test_data = pd.read_csv(\"/content/PubMedQA_dataset/test_sample_100.csv\")\n",
    "        SAMPLE_SIZE = min(20, len(test_data))  \n",
    "        print(f\" Loaded {len(test_data)} samples, evaluating {SAMPLE_SIZE}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Cannot load test data: {e}\")\n",
    "        return manual_non_rag_evaluation()\n",
    "\n",
    "    print(f\"üéØ NON-RAG REALISTIC Evaluation on {SAMPLE_SIZE} samples\")\n",
    "\n",
    "    results = []\n",
    "    success_count = 0\n",
    "\n",
    "    for idx, row in test_data.head(SAMPLE_SIZE).iterrows():\n",
    "        try:\n",
    "           \n",
    "            if idx % 5 == 0:\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(1)  \n",
    "\n",
    "            question = str(row[test_data.columns[0]])\n",
    "            if not question or len(question) < 10:\n",
    "                print(f\" Skip sample {idx+1}: question too short\")\n",
    "                continue\n",
    "\n",
    "            result = answer_biomedical_question_non_rag_safe(question)\n",
    "\n",
    "            results.append(result)\n",
    "            success_count += 1\n",
    "\n",
    "            print(f\" {idx+1}/{SAMPLE_SIZE}: {result['short_answer']} - {result['detailed_answer'][:50]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Sample {idx+1} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results, success_count\n",
    "\n",
    "def answer_biomedical_question_non_rag_safe(question):\n",
    "    \"\"\"Non-RAG pipeline an to√†n\"\"\"\n",
    "\n",
    "    try:\n",
    "        short_answer = classify_non_rag_answer(question)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Classification failed: {e}\")\n",
    "        short_answer = \"maybe\"  \n",
    "\n",
    "    detailed_answer = safe_non_rag_generate(question)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"short_answer\": short_answer,\n",
    "        \"detailed_answer\": detailed_answer,\n",
    "        \"retrieved_evidence\": [],\n",
    "        \"relevance_scores\": [],\n",
    "        \"method\": \"non_rag_safe\"\n",
    "    }\n",
    "\n",
    "def safe_non_rag_generate(question, max_retries=2):\n",
    "    \"\"\"Generation an to√†n cho Non-RAG\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "           \n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a professional medical assistant. Answer the biomedical question based on your knowledge.\n",
    "Provide a concise but informative answer with:\n",
    "- Clear yes/no/maybe indication\n",
    "- Brief explanation\n",
    "- Note uncertainties if any\n",
    "\n",
    "Question: {question}\n",
    "<</SYS>>\n",
    "\n",
    "Please provide your medical opinion: [/INST]\"\"\"\n",
    "\n",
    "            response = llm(\n",
    "                prompt,\n",
    "                max_tokens=200,   \n",
    "                temperature=0.1,   \n",
    "                top_p=0.9,\n",
    "                echo=False,\n",
    "                stop=[\"</s>\", \"[INST]\"],\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            answer = response['choices'][0]['text'].strip()\n",
    "\n",
    "            if len(answer) < 15:\n",
    "                raise ValueError(\"Generated answer too short\")\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Generation attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                return f\" Could not generate detailed answer. Short answer: {classify_non_rag_answer(question)}\"\n",
    "\n",
    "def classify_non_rag_answer(question):\n",
    "    \"\"\"Classification an to√†n\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer_classify(\n",
    "            question,\n",
    "            \"\",  \n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,  \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_classify(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "        label = \"yes\" if pred == 0 else \"no\" if pred == 1 else \"maybe\"\n",
    "        return label\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Classification error: {e}\")\n",
    "        return \"maybe\"  \n",
    "\n",
    "def manual_non_rag_evaluation():\n",
    "    \"\"\"ƒê√°nh gi√° th·ªß c√¥ng n·∫øu automatic failed\"\"\"\n",
    "    print(\"üîß Falling back to MANUAL Non-RAG evaluation...\")\n",
    "\n",
    "    sample_questions = [\n",
    "        \"Does aspirin reduce heart attack risk in diabetic patients?\",\n",
    "        \"Is metformin effective for weight loss?\",\n",
    "        \"Can vitamin D prevent respiratory infections?\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for q in sample_questions:\n",
    "        try:\n",
    "            result = answer_biomedical_question_non_rag_safe(q)\n",
    "            results.append(result)\n",
    "            print(f\" Manual: {result['short_answer']} - {result['detailed_answer'][:60]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\" Manual sample failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results, len(results)\n",
    "\n",
    "def analyze_non_rag_results(results):\n",
    "    \"\"\"Ph√¢n t√≠ch k·∫øt qu·∫£ Non-RAG\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ùå No results to analyze!\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n NON-RAG EVALUATION SUMMARY:\")\n",
    "    print(f\" Total successful: {len(results)} samples\")\n",
    "\n",
    "    short_answers = [r['short_answer'] for r in results]\n",
    "    answer_counts = {answer: short_answers.count(answer) for answer in set(short_answers)}\n",
    "    print(f\"üìà Answer distribution: {answer_counts}\")\n",
    "\n",
    "    avg_length = sum(len(r['detailed_answer']) for r in results) / len(results)\n",
    "    print(f\"üìè Average answer length: {avg_length:.1f} characters\")\n",
    "\n",
    "    print(f\"\\nüîç QUALITY SAMPLES:\")\n",
    "    for i, result in enumerate(results[:3]):\n",
    "        print(f\"\\n{i+1}. Q: {result['question'][:70]}...\")\n",
    "        print(f\"   Short: {result['short_answer']}\")\n",
    "        print(f\"   Detailed: {result['detailed_answer'][:100]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting REALISTIC NON-RAG evaluation...\")\n",
    "    results, success_count = robust_non_rag_evaluation()\n",
    "\n",
    "    analyze_non_rag_results(results)\n",
    "\n",
    "    if success_count > 0:\n",
    "        with open('/content/non_rag_realistic_results.json', 'w') as f:\n",
    "            json.dump({\n",
    "                \"evaluation_type\": \"non_rag_realistic\",\n",
    "                \"sample_count\": success_count,\n",
    "                \"results\": results\n",
    "            }, f, indent=2)\n",
    "        print(f\"\\n Non-RAG realistic results saved: {success_count} samples\")\n",
    "    else:\n",
    "        print(\" No successful evaluations to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    with open('/content/non_rag_realistic_results.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        results = data['results']  \n",
    "    print(f\" Loaded {len(results)} generation results\")\n",
    "except:\n",
    "    print(\" Cannot load generation results, using empty list\")\n",
    "    results = []\n",
    "\n",
    "print(f\"üìä Calculating STANDARD metrics on {len(results)} samples...\")\n",
    "\n",
    "try:\n",
    "    test_file_path = \"/content/PubMedQA_dataset/test_sample_100.csv\"\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    question_col = None\n",
    "    answer_col = None\n",
    "\n",
    "    for col in test_data.columns:\n",
    "        if any(keyword in col.lower() for keyword in ['question', 'text', 'q']):\n",
    "            question_col = col\n",
    "        elif any(keyword in col.lower() for keyword in ['answer', 'reference', 'ground']):\n",
    "            answer_col = col\n",
    "\n",
    "    if question_col is None:\n",
    "        question_col = test_data.columns[0]\n",
    "    if answer_col is None and len(test_data.columns) > 1:\n",
    "        answer_col = test_data.columns[1]\n",
    "\n",
    "    print(f\"üîç Detected columns - Question: {question_col}, Answer: {answer_col}\")\n",
    "\n",
    "    gt_mapping = {}\n",
    "    for idx, row in test_data.iterrows():\n",
    "        question = str(row[question_col]) if pd.notna(row[question_col]) else \"\"\n",
    "        if answer_col:\n",
    "            ground_truth = str(row[answer_col]) if pd.notna(row[answer_col]) else \"\"\n",
    "        else:\n",
    "            ground_truth = \"\"\n",
    "\n",
    "        if question:\n",
    "            gt_mapping[question.strip()] = ground_truth.strip()\n",
    "\n",
    "    print(f\" Loaded {len(gt_mapping)} ground truth mappings\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Cannot load ground truths: {e}\")\n",
    "    gt_mapping = {}\n",
    "\n",
    "generated_answers = []\n",
    "ground_truths = []\n",
    "valid_samples = []\n",
    "\n",
    "for result in results:\n",
    "    question = result['question'].strip()\n",
    "    generated = result['detailed_answer'].strip()\n",
    "\n",
    "    if question in gt_mapping:\n",
    "        gt = gt_mapping[question]\n",
    "    else:\n",
    "        gt = \"\"\n",
    "        for gt_question, gt_answer in gt_mapping.items():\n",
    "            if question[:30] in gt_question or gt_question[:30] in question:\n",
    "                gt = gt_answer\n",
    "                break\n",
    "\n",
    "    if gt:  \n",
    "        generated_answers.append(generated)\n",
    "        ground_truths.append(gt)\n",
    "        valid_samples.append({\n",
    "            'question': question,\n",
    "            'generated': generated,\n",
    "            'ground_truth': gt,\n",
    "            'short_answer': result.get('short_answer', 'unknown')\n",
    "        })\n",
    "\n",
    "print(f\"üîç Found {len(valid_samples)} valid samples with ground truth\")\n",
    "\n",
    "if len(valid_samples) == 0:\n",
    "    print(\" No ground truth matches found. Using short answers as proxy...\")\n",
    "    for result in results:\n",
    "        generated_answers.append(result['detailed_answer'])\n",
    "        short_answer = result.get('short_answer', 'maybe')\n",
    "        gt_proxy = f\"This is a {short_answer} answer according to the classification.\"\n",
    "        ground_truths.append(gt_proxy)\n",
    "        valid_samples.append({\n",
    "            'question': result['question'],\n",
    "            'generated': result['detailed_answer'],\n",
    "            'ground_truth': gt_proxy,\n",
    "            'short_answer': short_answer\n",
    "        })\n",
    "\n",
    "print(\"\\nüéØ CALCULATING ROUGE METRICS...\")\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for gen, gt in zip(generated_answers, ground_truths):\n",
    "    try:\n",
    "        scores = scorer.score(gt, gen)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    except Exception as e:\n",
    "        print(f\" ROUGE calculation error: {e}\")\n",
    "        rouge1_scores.append(0.0)\n",
    "        rouge2_scores.append(0.0)\n",
    "        rougeL_scores.append(0.0)\n",
    "\n",
    "rouge1_avg = np.mean(rouge1_scores) if rouge1_scores else 0.0\n",
    "rouge2_avg = np.mean(rouge2_scores) if rouge2_scores else 0.0\n",
    "rougeL_avg = np.mean(rougeL_scores) if rougeL_scores else 0.0\n",
    "\n",
    "print(\" CALCULATING BLEU METRICS...\")\n",
    "smooth = SmoothingFunction().method4\n",
    "bleu_scores = []\n",
    "\n",
    "for gen, gt in zip(generated_answers, ground_truths):\n",
    "    try:\n",
    "        reference = [gt.split()]\n",
    "        candidate = gen.split()\n",
    "\n",
    "        if len(reference[0]) > 0 and len(candidate) > 0:\n",
    "            bleu_score = sentence_bleu(reference, candidate, smoothing_function=smooth)\n",
    "            bleu_scores.append(bleu_score)\n",
    "        else:\n",
    "            bleu_scores.append(0.0)\n",
    "    except Exception as e:\n",
    "        bleu_scores.append(0.0)\n",
    "\n",
    "bleu_avg = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "print(\" CALCULATING BERTScore...\")\n",
    "try:\n",
    "    P, R, F1 = bert_score.score(generated_answers, ground_truths, lang=\"en\", verbose=True)\n",
    "    bertscore_avg = F1.mean().item()\n",
    "    bertscore_std = F1.std().item()\n",
    "except Exception as e:\n",
    "    print(f\" BERTScore calculation failed: {e}\")\n",
    "    bertscore_avg = 0.0\n",
    "    bertscore_std = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" STANDARD GENERATION METRICS - NON-RAG SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Sample Size: {len(valid_samples)}\")\n",
    "print(f\" ROUGE-1:  {rouge1_avg:.4f} (¬±{np.std(rouge1_scores):.4f})\")\n",
    "print(f\" ROUGE-2:  {rouge2_avg:.4f} (¬±{np.std(rouge2_scores):.4f})\")\n",
    "print(f\" ROUGE-L:  {rougeL_avg:.4f} (¬±{np.std(rougeL_scores):.4f})\")\n",
    "print(f\" BLEU:     {bleu_avg:.4f} (¬±{np.std(bleu_scores):.4f})\")\n",
    "print(f\" BERTScore: {bertscore_avg:.4f} (¬±{bertscore_std:.4f})\")\n",
    "\n",
    "if valid_samples:\n",
    "    short_answers = [s['short_answer'] for s in valid_samples]\n",
    "    answer_counts = {}\n",
    "    for ans in short_answers:\n",
    "        answer_counts[ans] = answer_counts.get(ans, 0) + 1\n",
    "    print(f\"üìä Answer Distribution: {answer_counts}\")\n",
    "\n",
    "print(f\"\\nüîç SAMPLE COMPARISONS:\")\n",
    "for i in range(min(3, len(valid_samples))):\n",
    "    sample = valid_samples[i]\n",
    "    print(f\"\\n{i+1}. Q: {sample['question'][:60]}...\")\n",
    "    print(f\"   Generated: {sample['generated'][:80]}...\")\n",
    "    print(f\"   Ground Truth: {sample['ground_truth'][:80]}...\")\n",
    "    print(f\"   Metrics - ROUGE-1: {rouge1_scores[i]:.3f}, BLEU: {bleu_scores[i]:.3f}\")\n",
    "\n",
    "detailed_metrics = {\n",
    "    'summary': {\n",
    "        'rouge1': rouge1_avg,\n",
    "        'rouge2': rouge2_avg,\n",
    "        'rougeL': rougeL_avg,\n",
    "        'bleu': bleu_avg,\n",
    "        'bertscore': bertscore_avg,\n",
    "        'sample_size': len(valid_samples),\n",
    "        'answer_distribution': answer_counts if valid_samples else {}\n",
    "    },\n",
    "    'per_sample_scores': [\n",
    "        {\n",
    "            'question': sample['question'],\n",
    "            'generated': sample['generated'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'short_answer': sample['short_answer'],\n",
    "            'rouge1': rouge1_scores[i],\n",
    "            'rouge2': rouge2_scores[i],\n",
    "            'rougeL': rougeL_scores[i],\n",
    "            'bleu': bleu_scores[i] if i < len(bleu_scores) else 0.0\n",
    "        }\n",
    "        for i, sample in enumerate(valid_samples)\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('/content/non_rag_standard_metrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(detailed_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n Standard metrics saved to: /content/non_rag_standard_metrics.json\")\n",
    "print(\" Evaluation with STANDARD metrics completed! \")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
